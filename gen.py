import json
import vertexai
from vertexai.generative_models import GenerativeModel, Part
import vertexai.preview.generative_models as generative_models
from google.cloud import storage
import os
import requests  # Import the requests library



def generate_content(url):
    vertexai.init(project="project-4-workndemos", location="us-east1")
    
    
    model = GenerativeModel(
        "gemini-1.5-pro-001"
    )

    document1 = Part.from_uri(mime_type="application/pdf", uri=url)

    message = """
   extract the information in json format with cloumns employee name, designation and netpay 
    """

    generation_config = {
        "temperature": 0.2,
    }

    safety_settings = {
        generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
        generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
        generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
        generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    }

    print(f"Processing: {url} now without streaming (individual document processing).")
    
    # Stream is set to False to treat each part as a separate document
    response = model.generate_content(
        [message, document1],
        generation_config=generation_config,
        safety_settings=safety_settings,
        stream=False  # Disable streaming mode to process parts individually
    )
    # print(response.text)

    # Since response is not iterable when stream=False, directly access the response text
    output = response.text if hasattr(response, 'text') else response

    combined_text = output.strip("```json").strip("```").strip()

    if not combined_text:
        raise ValueError("The AI model did not return any valid content to process.")

    try:
        # Attempt to load the combined JSON text
        parsed_data = json.loads(combined_text)

        # If it's a list of dictionaries, return it directly
        if isinstance(parsed_data, list):
            return parsed_data
        # Otherwise, return it as a single-item list
        else:
            return [parsed_data]

    except json.JSONDecodeError as e:
        print(f"Failed to decode JSON. Error: {e}")
        print(f"Raw response content: {combined_text}")
        raise

def upload_to_gcs(bucket_name, destination_blob_name, content):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_string(content, content_type='application/json')
    print(f"Content uploaded to {bucket_name}/{destination_blob_name}")

def extract_and_upload(urls, source_blob_name):
    bucket_name = "as_pdf"
    bucket_name1 = "as_pdf"
    all_extracted_data = []
    print(f"Now in extract_and_upload() with URLs: {urls}")

    for index, url in enumerate(urls, start=1):
        try:
            print(f"Working on: {url}")
            extracted_data = generate_content(url)
            if extracted_data:
                part_file_name = f"{os.path.basename(source_blob_name).replace('.pdf', '')}_part_{index}.json"
                part_json = json.dumps(extracted_data, indent=2)
                upload_to_gcs(bucket_name1, part_file_name, part_json)
                all_extracted_data.extend(extracted_data)
            else:
                print(f"No content generated by the AI model for {url}.")
        except Exception as e:
            print(f"An error occurred during content generation: {e}")

    if all_extracted_data:
        output_json = json.dumps(all_extracted_data, indent=2)
        combined_file_name = os.path.basename(source_blob_name).replace('.pdf', '.json')
        print(f"Uploading combined JSON: {combined_file_name}")
        upload_to_gcs(bucket_name, combined_file_name, output_json)
    else:
        print("No data to upload after processing all parts.")